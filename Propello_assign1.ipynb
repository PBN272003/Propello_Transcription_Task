{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMSw5iIuH7iT",
        "outputId": "ac010eb7-3a48-488f-cac3-d936b4970e13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchaudio speechbrain librosa numpy streamlit streamlit_chat torch transformers faiss-cpu google-generativeai flashrank langchain pydantic\n",
        "!pip install python-docx tiktoken torch torchaudio librosa ffmpeg-python speechbrain scipy tqdm  json5 pyngrok\n",
        "!pip install git+https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzhTrT3bMYHV",
        "outputId": "35869305-a171-428a-e616-fa07b2bdf16f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: speechbrain in /usr/local/lib/python3.11/dist-packages (1.0.3)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.47.1)\n",
            "Requirement already satisfied: streamlit_chat in /usr/local/lib/python3.11/dist-packages (0.1.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: flashrank in /usr/local/lib/python3.11/dist-packages (0.2.10)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.11.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: hyperpyyaml in /usr/local/lib/python3.11/dist-packages (from speechbrain) (1.2.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from speechbrain) (1.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from speechbrain) (25.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from speechbrain) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from speechbrain) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from speechbrain) (4.67.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from speechbrain) (0.34.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.177.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.11/dist-packages (from flashrank) (1.22.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.72)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.8)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.4.1)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.48.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->speechbrain) (1.1.5)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: ruamel.yaml>=0.17.28 in /usr/local/lib/python3.11/dist-packages (from hyperpyyaml->speechbrain) (0.18.14)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime->flashrank) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime->flashrank) (25.2.10)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.11/dist-packages (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain) (0.2.12)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime->flashrank) (10.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: ffmpeg-python in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: speechbrain in /usr/local/lib/python3.11/dist-packages (1.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: json5 in /usr/local/lib/python3.11/dist-packages (0.12.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.12)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from librosa) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from ffmpeg-python) (1.0.0)\n",
            "Requirement already satisfied: hyperpyyaml in /usr/local/lib/python3.11/dist-packages (from speechbrain) (1.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from speechbrain) (25.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from speechbrain) (0.2.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from speechbrain) (0.34.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.7.14)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->speechbrain) (1.1.5)\n",
            "Requirement already satisfied: ruamel.yaml>=0.17.28 in /usr/local/lib/python3.11/dist-packages (from hyperpyyaml->speechbrain) (0.18.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.11/dist-packages (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain) (0.2.12)\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-i_pw1q0d\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-i_pw1q0d\n",
            "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (10.7.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (0.9.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20250625) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20250625) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20250625) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20250625) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20250625) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import librosa\n",
        "from speechbrain.inference import SpeakerRecognition\n",
        "import tempfile\n",
        "import random\n",
        "\n",
        "# Check if the required directories exist; create them if not\n",
        "def create_directory(directory):\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "# Enhanced function to load audio files and extract embeddings\n",
        "def load_audio_and_extract_embeddings(audio_file_path):\n",
        "    # Load the audio file\n",
        "    signal, sample_rate = torchaudio.load(audio_file_path)\n",
        "\n",
        "    # Ensure the audio is mono\n",
        "    if signal.shape[0] > 1:\n",
        "        signal = signal.mean(dim=0, keepdim=True)\n",
        "\n",
        "    # Resample if necessary\n",
        "    if sample_rate != 16000:\n",
        "        signal = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(signal)\n",
        "\n",
        "    # Apply pre-emphasis filter to enhance speech features\n",
        "    signal_np = signal.squeeze().numpy()\n",
        "    emphasized_signal = np.append(signal_np[0], signal_np[1:] - 0.97 * signal_np[:-1])\n",
        "    signal = torch.FloatTensor(emphasized_signal).unsqueeze(0)\n",
        "\n",
        "    # Voice activity detection to remove silence\n",
        "    intervals = librosa.effects.split(signal_np, top_db=20)\n",
        "    if len(intervals) > 0:\n",
        "        y_vad = np.concatenate([signal_np[start:end] for start, end in intervals])\n",
        "        signal = torch.FloatTensor(y_vad).unsqueeze(0)\n",
        "\n",
        "    return signal, 16000\n",
        "\n",
        "# Data augmentation function\n",
        "def augment_audio(signal, sample_rate):\n",
        "    augmented_signals = [signal]  # Start with original signal\n",
        "    signal_np = signal.squeeze().numpy()\n",
        "\n",
        "    # Speed perturbation (time stretching)\n",
        "    for rate in [0.9, 1.1]:  # 10% slower and 10% faster\n",
        "        y_stretched = librosa.effects.time_stretch(signal_np, rate=rate)\n",
        "        augmented_signals.append(torch.FloatTensor(y_stretched).unsqueeze(0))\n",
        "\n",
        "    # Pitch shifting (shifts the pitch without changing tempo)\n",
        "    for n_steps in [-2, 2]:  # Shift pitch down and up by 2 semitones\n",
        "        y_shifted = librosa.effects.pitch_shift(signal_np, sr=sample_rate, n_steps=n_steps)\n",
        "        augmented_signals.append(torch.FloatTensor(y_shifted).unsqueeze(0))\n",
        "\n",
        "    # Adding noise at different SNRs\n",
        "    for snr_db in [20, 15]:  # Signal-to-noise ratio in dB\n",
        "        noise = np.random.randn(len(signal_np))\n",
        "        signal_power = np.mean(signal_np ** 2)\n",
        "        noise_power = np.mean(noise ** 2)\n",
        "\n",
        "        # Calculate the factor to scale noise to desired SNR\n",
        "        snr = 10 ** (snr_db / 10)\n",
        "        scale = np.sqrt(signal_power / (noise_power * snr))\n",
        "\n",
        "        noisy_signal = signal_np + scale * noise\n",
        "        augmented_signals.append(torch.FloatTensor(noisy_signal).unsqueeze(0))\n",
        "\n",
        "    # Random volume change\n",
        "    for volume in [0.8, 1.2]:  # 20% quieter and 20% louder\n",
        "        augmented_signals.append(signal * volume)\n",
        "\n",
        "    return augmented_signals\n",
        "\n",
        "# Improved training function with data augmentation\n",
        "def train_model(audio_files, labels):\n",
        "    classifier = SpeakerRecognition.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\", savedir=\"tmpdir_ecapa\")\n",
        "\n",
        "    # Iterate over each audio file\n",
        "    for audio_file, label in zip(audio_files, labels):\n",
        "        print(f\"Processing {label} from {audio_file}\")\n",
        "        signal, sample_rate = load_audio_and_extract_embeddings(audio_file)\n",
        "\n",
        "        # Get duration of the audio\n",
        "        segments_duration = len(signal.squeeze()) / sample_rate\n",
        "        print(f\"Duration of {audio_file}: {segments_duration:.2f} seconds\")\n",
        "\n",
        "        # Apply data augmentation\n",
        "        augmented_signals = augment_audio(signal, sample_rate)\n",
        "        print(f\"Created {len(augmented_signals)} augmented versions (including original)\")\n",
        "\n",
        "        all_embeddings = []\n",
        "\n",
        "        # Process each signal (original and augmented)\n",
        "        for idx, aug_signal in enumerate(augmented_signals):\n",
        "            # For longer recordings, use chunking for more robust embeddings\n",
        "            if len(aug_signal.squeeze()) > 5 * sample_rate:  # If longer than 5 seconds\n",
        "                chunk_length = int(3 * sample_rate)  # 3-second chunks\n",
        "                hop_length = int(1.5 * sample_rate)  # 50% overlap\n",
        "\n",
        "                chunks = []\n",
        "                aug_signal_np = aug_signal.squeeze().numpy()\n",
        "\n",
        "                # Create overlapping chunks\n",
        "                for i in range(0, len(aug_signal_np) - chunk_length + 1, hop_length):\n",
        "                    chunk = aug_signal_np[i:i + chunk_length]\n",
        "                    chunks.append(torch.FloatTensor(chunk).unsqueeze(0))\n",
        "\n",
        "                # Get embedding for each chunk\n",
        "                chunk_embeddings = []\n",
        "                for chunk in chunks:\n",
        "                    emb = classifier.encode_batch(chunk).squeeze().detach().numpy()\n",
        "                    chunk_embeddings.append(emb)\n",
        "\n",
        "                # Average the embeddings for a more robust representation\n",
        "                if chunk_embeddings:\n",
        "                    emb = np.mean(np.array(chunk_embeddings), axis=0)\n",
        "                    all_embeddings.append(emb)\n",
        "            else:\n",
        "                # For shorter recordings, just get the single embedding\n",
        "                try:\n",
        "                    emb = classifier.encode_batch(aug_signal.unsqueeze(0)).squeeze().detach().numpy()\n",
        "                    all_embeddings.append(emb)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing augmented signal {idx}: {str(e)}\")\n",
        "\n",
        "        # Average all embeddings (original and augmented)\n",
        "        if all_embeddings:\n",
        "            segment_embedding = np.mean(np.array(all_embeddings), axis=0)\n",
        "\n",
        "            # Normalize the embedding (important for cosine similarity)\n",
        "            segment_embedding = segment_embedding / np.linalg.norm(segment_embedding)\n",
        "\n",
        "            # Save embedding as .npy file\n",
        "            np.save(f\"embeddings4/{label}.npy\", segment_embedding)\n",
        "            print(f\"Saved enhanced embedding for {label} combining {len(all_embeddings)} variants\")\n",
        "        else:\n",
        "            print(f\"Warning: No successful embeddings generated for {label}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Create necessary directories\n",
        "    create_directory(\"/content/embeddings4\")\n",
        "\n",
        "    # Example audio files and labels\n",
        "    audio_files = [\n",
        "        \"/content/drive/MyDrive/PropelloAI_assign-1/nisha_audio.mp3\"\n",
        "    ]\n",
        "\n",
        "    labels = [\n",
        "    \"Nisha\"]\n",
        "\n",
        "\n",
        "    # Train the model\n",
        "    train_model(audio_files, labels)\n"
      ],
      "metadata": {
        "id": "fOdogULbmctw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75b590ac-3e23-4a69-c50f-6980fcbea644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Using symlink found at '/content/tmpdir_ecapa/hyperparams.yaml'\n",
            "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Collecting files (or symlinks) for pretraining in tmpdir_ecapa.\n",
            "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Using symlink found at '/content/tmpdir_ecapa/embedding_model.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"embedding_model\"] = /content/tmpdir_ecapa/embedding_model.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Using symlink found at '/content/tmpdir_ecapa/mean_var_norm_emb.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"mean_var_norm_emb\"] = /content/tmpdir_ecapa/mean_var_norm_emb.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Using symlink found at '/content/tmpdir_ecapa/classifier.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"classifier\"] = /content/tmpdir_ecapa/classifier.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Using symlink found at '/content/tmpdir_ecapa/label_encoder.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"label_encoder\"] = /content/tmpdir_ecapa/label_encoder.ckpt\n",
            "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): embedding_model -> /content/tmpdir_ecapa/embedding_model.ckpt\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): mean_var_norm_emb -> /content/tmpdir_ecapa/mean_var_norm_emb.ckpt\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): classifier -> /content/tmpdir_ecapa/classifier.ckpt\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): label_encoder -> /content/tmpdir_ecapa/label_encoder.ckpt\n",
            "DEBUG:speechbrain.dataio.encoder:Loaded categorical encoding from /content/tmpdir_ecapa/label_encoder.ckpt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Nisha from /content/drive/MyDrive/PropelloAI_assign-1/nisha_audio.mp3\n",
            "Duration of /content/drive/MyDrive/PropelloAI_assign-1/nisha_audio.mp3: 10.43 seconds\n",
            "Created 9 augmented versions (including original)\n",
            "Saved enhanced embedding for Nisha combining 9 variants\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from docx import Document\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "import numpy as np\n",
        "import torchaudio\n",
        "import pathlib\n",
        "import tempfile\n",
        "import librosa\n",
        "import ffmpeg\n",
        "import streamlit as st\n",
        "from streamlit_chat import message\n",
        "from tqdm import tqdm\n",
        "from functools import lru_cache\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.spatial.distance import cosine\n",
        "from speechbrain.inference import SpeakerRecognition\n",
        "import whisper\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import faiss\n",
        "import google.generativeai as genai\n",
        "from flashrank.Ranker import Ranker, RerankRequest\n",
        "from io import BytesIO\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "import time\n",
        "import requests\n",
        "from speechbrain.inference.VAD import VAD\n",
        "start_time = time.time()\n",
        "genai.configure(api_key=\"AIzaSyArG3gnpZHnzi10mMSnyOMhzYJBeAZEJUs\")\n",
        "st.set_page_config(page_title=\"AI Meeting Assistant\", layout=\"wide\")\n",
        "st.title(\"ðŸ“„ AI-Powered Meeting Assistant\")\n",
        "# Caching Whisper and SpeakerRecognition models\n",
        "@lru_cache(maxsize=1)\n",
        "def load_whisper_model():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    return whisper.load_model(\"large\", device=device)\n",
        "\n",
        "@lru_cache(maxsize=1)\n",
        "def load_classifier_model():\n",
        "    return SpeakerRecognition.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\", savedir=\"tmpdir_ecapa\")\n",
        "\n",
        "def load_embeddings(embedding_dir):\n",
        "    embeddings = {}\n",
        "    for file in os.listdir(embedding_dir):\n",
        "        if file.endswith(\".npy\"):\n",
        "            speaker_label = file.split(\".npy\")[0]\n",
        "            embeddings[speaker_label] = np.load(os.path.join(embedding_dir, file))\n",
        "    return embeddings\n",
        "\n",
        "def segment_audio(signal, sample_rate, segment_duration=0.3):\n",
        "    chunk_samples = int(segment_duration * sample_rate)\n",
        "    segments = []\n",
        "    for start in range(0, len(signal), chunk_samples):\n",
        "        end = start + chunk_samples\n",
        "        segment = signal[start:end]\n",
        "        if len(segment) == chunk_samples:\n",
        "            start_time = start/sample_rate\n",
        "            end_time = end/sample_rate\n",
        "            segments.append((start_time,end_time,segment))\n",
        "    return segments\n",
        "def diarize_and_transcribe(test_audio_path, embeddings, chunk_duration=3.0, transcribe_duration=75.0, similarity_threshold=0.35):\n",
        "    # Load audio\n",
        "    signal, sample_rate = torchaudio.load(test_audio_path)\n",
        "    if signal.shape[0] > 1:\n",
        "        signal = signal.mean(dim=0, keepdim=True)\n",
        "    if sample_rate != 16000:\n",
        "        signal = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(signal)\n",
        "    sample_rate = 16000\n",
        "    # Init\n",
        "    tmpdir = pathlib.Path(tempfile.mkdtemp())\n",
        "    whisper_model = whisper.load_model(\"medium\")\n",
        "    classifier = load_classifier_model()\n",
        "    diarization_result = []\n",
        "    all_segments = []\n",
        "\n",
        "    # Process chunks\n",
        "    total_duration = librosa.get_duration(y=signal.numpy(), sr=sample_rate)\n",
        "    num_chunks = int(total_duration // transcribe_duration) + 1\n",
        "\n",
        "    for i in tqdm(range(num_chunks), desc=\"Chunking\"):\n",
        "        start_time = i * transcribe_duration\n",
        "        chunk_path = tmpdir / f\"chunk_{i}.wav\"\n",
        "\n",
        "        stream = ffmpeg.input(test_audio_path, ss=start_time, t=transcribe_duration)\n",
        "        stream = stream.output(str(chunk_path), format='wav', acodec='pcm_s16le', ac=1, ar='16k').overwrite_output()\n",
        "        ffmpeg.run(stream, capture_stdout=True, capture_stderr=True)\n",
        "\n",
        "        chunk_signal, _ = torchaudio.load(str(chunk_path))\n",
        "        segments = segment_audio(chunk_signal.squeeze(0), sample_rate, segment_duration=chunk_duration)\n",
        "\n",
        "        for seg_start, seg_end, segment in segments:\n",
        "            full_start = start_time + seg_start\n",
        "            full_end = start_time + seg_end\n",
        "\n",
        "            with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as temp_wav:\n",
        "                torchaudio.save(temp_wav.name, segment.unsqueeze(0), sample_rate=sample_rate)\n",
        "                result = whisper_model.transcribe(temp_wav.name, task='translate')\n",
        "                text = result['text']\n",
        "\n",
        "            emb = classifier.encode_batch(segment.unsqueeze(0)).squeeze().detach().numpy()\n",
        "            emb = emb / np.linalg.norm(emb)\n",
        "\n",
        "            all_segments.append({\n",
        "                \"start_time\": full_start,\n",
        "                \"end_time\": full_end,\n",
        "                \"embedding\": emb,\n",
        "                \"text\": text\n",
        "            })\n",
        "\n",
        "    # === CLUSTERING STAGE ===\n",
        "    all_embeddings = np.array([s[\"embedding\"] for s in all_segments])\n",
        "    #reduced_embeddings = PCA(n_components=min(64, all_embeddings.shape[1])).fit_transform(all_embeddings)\n",
        "\n",
        "    clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=0.5, linkage='average')\n",
        "    cluster_labels = clustering.fit_predict(all_embeddings)\n",
        "\n",
        "    # Assign labels\n",
        "    known_embeddings = {k: v / np.linalg.norm(v) for k, v in embeddings.items()}\n",
        "    cluster_to_label = {}\n",
        "    unknown_count = 1\n",
        "\n",
        "    for cluster_id in set(cluster_labels):\n",
        "        indices = [i for i, label in enumerate(cluster_labels) if label == cluster_id]\n",
        "        cluster_embs = np.array([all_segments[i][\"embedding\"] for i in indices])\n",
        "        cluster_centroid = np.mean(cluster_embs, axis=0)\n",
        "        cluster_centroid /= np.linalg.norm(cluster_centroid)\n",
        "\n",
        "        best_match = None\n",
        "        best_score = -1\n",
        "        for speaker, known_emb in known_embeddings.items():\n",
        "            score = 1 - cosine(cluster_centroid, known_emb)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_match = speaker\n",
        "\n",
        "        if best_score >= similarity_threshold:\n",
        "            cluster_to_label[cluster_id] = best_match\n",
        "        else:\n",
        "            cluster_to_label[cluster_id] = f\"unknown_speaker_{unknown_count}\"\n",
        "            unknown_count += 1\n",
        "\n",
        "    # Assign speaker labels to segments\n",
        "    for idx, segment in enumerate(all_segments):\n",
        "        cluster_id = cluster_labels[idx]\n",
        "        diarization_result.append({\n",
        "            \"start_time\": segment[\"start_time\"],\n",
        "            \"end_time\": segment[\"end_time\"],\n",
        "            \"speaker\": cluster_to_label[cluster_id],\n",
        "            \"text\": segment[\"text\"],\n",
        "            \"similarity_score\": None,\n",
        "            \"all_similarity_scores\": {}\n",
        "        })\n",
        "\n",
        "    # Clean up\n",
        "    for chunk in tmpdir.iterdir():\n",
        "        chunk.unlink()\n",
        "    tmpdir.rmdir()\n",
        "\n",
        "    return sorted(diarization_result, key=lambda x: x[\"start_time\"])\n",
        "\n",
        "def merge_consecutive_segments(diarization_result, max_gap=0.5):\n",
        "    merged_result = []\n",
        "    current_segment = None\n",
        "\n",
        "    for segment in diarization_result:\n",
        "        if current_segment is None:\n",
        "            current_segment = segment\n",
        "        else:\n",
        "            gap = segment[\"start_time\"] - current_segment[\"end_time\"]\n",
        "            if current_segment[\"speaker\"] == segment[\"speaker\"] and gap <= max_gap:\n",
        "                current_segment[\"end_time\"] = segment[\"end_time\"]\n",
        "                current_segment[\"text\"] += \" \" + segment[\"text\"]\n",
        "            else:\n",
        "                merged_result.append(current_segment)\n",
        "                current_segment = segment\n",
        "\n",
        "    if current_segment is not None:\n",
        "        merged_result.append(current_segment)\n",
        "\n",
        "    return merged_result\n",
        "def extract_text_from_docx(docx_file):\n",
        "    \"\"\"Extracts text from a reference MoM document.\"\"\"\n",
        "    doc = Document(docx_file)\n",
        "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "\n",
        "def convert_numpy_types(obj):\n",
        "    if isinstance(obj, np.generic):\n",
        "        return obj.item()\n",
        "    elif isinstance(obj, dict):\n",
        "        return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [convert_numpy_types(element) for element in obj]\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "def generate_mom_with_gemini(diarization_json_path, reference_mom, present_speakers, all_speakers):\n",
        "    \"\"\"Generates MoM in structured format using Gemini AI.\"\"\"\n",
        "    with open(diarization_json_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Speaker-wise meeting summary\n",
        "    discussions = [f\"{item['speaker']}: {item['text']}\" for item in data]\n",
        "    full_text = \"\\n\".join(discussions)\n",
        "\n",
        "    # Determine absentees\n",
        "    absent_speakers = [s for s in all_speakers if s not in present_speakers]\n",
        "\n",
        "    # Format attendees and absentees\n",
        "    attendees_list = \"\\n\".join([f\"- {name}\" for name in present_speakers])\n",
        "    absentees_list = \"\\n\".join([f\"- {name}\" for name in absent_speakers])\n",
        "\n",
        "    # AI Prompt for MoM\n",
        "    prompt = f\"\"\"\n",
        "    You are given a list of conversation segments with speaker labels and their spoken text. Your task is to generate a professionally structured *Transcript Document* that matches the exact formatting style of the provided reference transcript.\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### Reference Document Style:\n",
        "    {reference_mom}\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### Formatting Instructions:\n",
        "    1. **Heading and Date** â€“ Include a clear heading for the transcript and todayâ€™s date at the top.\n",
        "    2. **NOTICE Section** â€“ Include a short formal notice introducing the nature of the transcript (e.g., \"This transcript captures a conversation between the listed participants.\").\n",
        "    3. **AGENDA FOR THE CONVERSATION** â€“ Present in the following order:\n",
        "        - **List of Attendees:**\n",
        "    {attendees_list}\n",
        "\n",
        "        - **List of Absentees:**\n",
        "    {absentees_list}\n",
        "\n",
        "    4. **Conversation Format** â€“ Follow these strictly:\n",
        "        - Start each speaker turn with the speakerâ€™s name followed by a colon (`Speaker:`)\n",
        "        - The spoken dialogue must appear on the *same line* after the colon\n",
        "        - Insert a blank line between each speaker turn\n",
        "        - Keep speaker names exactly as provided (e.g., \"Bot\", \"User\", \"Rashmi\", etc.)\n",
        "        - Do **not** merge, summarize, or omit any utterances\n",
        "        - Do **not** include timestamps or any technical metadata\n",
        "        - Do **not** correct grammar or punctuation â€” preserve the speakerâ€™s original style\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### Now generate the final transcript for the following conversation:\n",
        "    {full_text}\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "    response = model.generate_content(prompt)\n",
        "    mom_text = response.text\n",
        "\n",
        "    # Create MoM Document\n",
        "    doc = Document()\n",
        "    doc.add_heading('Conversation Transcript', level=1)\n",
        "    doc.add_paragraph(mom_text)\n",
        "\n",
        "    mom_buffer = BytesIO()\n",
        "    doc.save(mom_buffer)\n",
        "    mom_buffer.seek(0)\n",
        "    return mom_buffer\n",
        "\n",
        "\n",
        "\n",
        "uploaded_mom = st.file_uploader(\"Upload a reference Transcript (Optional)\", type=[\"docx\"])\n",
        "reference_mom = \"\"\n",
        "if uploaded_mom:\n",
        "    reference_mom = extract_text_from_docx(uploaded_mom)\n",
        "uploaded_file = st.file_uploader(\"Upload an audio file\", type=[\"wav\", \"mp3\"])\n",
        "embeddings = load_embeddings(\"/content/embeddings4\")\n",
        "st.sidebar.header(\"ðŸ”Š Select Speakers for Matching\")\n",
        "selected_speakers = []\n",
        "for speaker in sorted(embeddings.keys()):\n",
        "  if st.sidebar.checkbox(speaker, value=True):\n",
        "    selected_speakers.append(speaker)\n",
        "selected_embeddings = {k: v for k, v in embeddings.items() if k in selected_speakers}\n",
        "all_embeddings = load_embeddings(\"/content/embeddings4\")\n",
        "if uploaded_file and \"processed_file\" not in st.session_state:\n",
        "    with st.spinner(\"Processing audio...\"):\n",
        "        temp_audio_path = f\"/tmp/{uploaded_file.name}\"\n",
        "        with open(temp_audio_path, \"wb\") as f:\n",
        "            f.write(uploaded_file.read())\n",
        "\n",
        "        diarization_result = diarize_and_transcribe(temp_audio_path, selected_embeddings, chunk_duration=5.0, transcribe_duration=75.0)\n",
        "        merged_result = merge_consecutive_segments(diarization_result)\n",
        "\n",
        "        output_json_path = \"diarization_result4.json\"\n",
        "        with open(output_json_path, \"w\") as json_file:\n",
        "            json.dump(merged_result, json_file, indent=4)\n",
        "\n",
        "    st.session_state[\"processed_file\"] = output_json_path\n",
        "    st.success(\"Diarization completed!\")\n",
        "\n",
        "    if reference_mom:\n",
        "        # ðŸ”¥ Pass present and all speaker names here\n",
        "        all_speakers = sorted(all_embeddings.keys())\n",
        "        mom_doc = generate_mom_with_gemini(output_json_path, reference_mom, selected_speakers, all_speakers)\n",
        "        st.download_button(\"ðŸ“¥ Download the transcript\", mom_doc, file_name=\"conversation.docx\")\n",
        "    else:\n",
        "        st.warning(\"âš ï¸ Please upload a reference MoM for better formatting.\")\n",
        "\n",
        "\n",
        "\n",
        "    # Load data and generate embeddings only once\n",
        "# if __name__ == \"__main__\":\n",
        "#     audio_path = \"/content/drive/MyDrive/FYP_MoM-20250424T113508Z-001/FYP_MoM/Fyp_meeting.mp3\"\n",
        "#     output_json = \"diarization_result.json\"\n",
        "#     # Load embeddings\n",
        "#     embeddings = load_embeddings(\"/content/embeddings4\")\n",
        "#     diarization_result = diarize_and_transcribe(audio_path, embeddings, chunk_duration=5.0, transcribe_duration=75.0)\n",
        "#     merged_result = merge_consecutive_segments(diarization_result)\n",
        "#     save_diarization_result(merged_result, output_json)\n"
      ],
      "metadata": {
        "id": "-WLqtiF40_rj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99d7e83c-8fae-491e-9fef-280a090eb5d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill -9 -f ngrok  # Use -9 for forceful termination if necessary"
      ],
      "metadata": {
        "id": "JVWfS--ZRdh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup streamlit run app.py --server.port 8502 --server.headless true > logs.txt 2>&1 &"
      ],
      "metadata": {
        "id": "HTL-0te6Redz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"2wAkGuxj4OJCPf0uZupbleH2FQs_7UANoLea7orq8ibFPHVnb\")\n",
        "public_url = ngrok.connect(8502)\n",
        "print(f\"Streamlit app is live at: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hge_u8IRgmI",
        "outputId": "79a8a5ec-bf48-4aa2-db23-cf8850ad584d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app is live at: NgrokTunnel: \"https://1d174116550f.ngrok-free.app\" -> \"http://localhost:8502\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"priyank.naik2003@gmail.com\"\n",
        "!git config --global user.name \"PBN272003\"\n",
        "\n",
        "# Clone your repo\n",
        "!git clone https://github.com/PBN272003/Propello_Transcription_Task.git\n",
        "%cd Propello_Transcription_Task\n",
        "\n",
        "# Save your work here\n",
        "# For example, saving notebook:\n",
        "!cp /content/Propello_assign1.ipynb .\n",
        "\n",
        "# Commit and push\n",
        "!git add Propello_assign1.ipynb\n",
        "!git commit -m \"Add notebook\"\n",
        "!git push"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNhEsmXX1qEF",
        "outputId": "ef9cd27e-2c67-4ae1-995b-bfe0a4207b6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Propello_Transcription_Task'...\n",
            "warning: You appear to have cloned an empty repository.\n",
            "/content/Propello_Transcription_Task\n",
            "cp: cannot stat '/content/Propello_assign1.ipynb': No such file or directory\n",
            "fatal: pathspec 'Propello_assign1.ipynb' did not match any files\n",
            "On branch main\n",
            "\n",
            "Initial commit\n",
            "\n",
            "nothing to commit (create/copy files and use \"git add\" to track)\n",
            "error: src refspec refs/heads/main does not match any\n",
            "\u001b[31merror: failed to push some refs to 'https://github.com/PBN272003/Propello_Transcription_Task.git'\n",
            "\u001b[m"
          ]
        }
      ]
    }
  ]
}